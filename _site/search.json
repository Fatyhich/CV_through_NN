[
  {
    "objectID": "lectures/1.html",
    "href": "lectures/1.html",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "",
    "text": "We will treat all vectors as column vectors by default. The space of real vectors of length \\(n\\) is denoted by \\(\\mathbb{R}^n\\), while the space of real-valued \\(m \\times n\\) matrices is denoted by \\(\\mathbb{R}^{m \\times n}\\). That’s it: 1\n\\[\nx = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix} \\quad x^T = \\begin{bmatrix}\nx_1 & x_2 & \\dots & x_n\n\\end{bmatrix} \\quad x \\in \\mathbb{R}^n, x_i \\in \\mathbb{R}\n\\tag{1}\\]\n. . .\nSimilarly, if \\(A \\in \\mathbb{R}^{m \\times n}\\) we denote transposition as \\(A^T \\in \\mathbb{R}^{n \\times m}\\): \\[\nA = \\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn}\n\\end{bmatrix} \\quad A^T = \\begin{bmatrix}\na_{11} & a_{21} & \\dots & a_{m1} \\\\\na_{12} & a_{22} & \\dots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\dots & a_{mn}\n\\end{bmatrix} \\quad A \\in \\mathbb{R}^{m \\times n}, a_{ij} \\in \\mathbb{R}\n\\] We will write \\(x \\geq 0\\) and \\(x \\neq 0\\) to indicate componentwise relationships\n\n\n\n\n\n\n\nFigure 1: Equivivalent representations of a vector\n\n\n\n\nA matrix is symmetric if \\(A = A^T\\). It is denoted as \\(A \\in \\mathbb{S}^n\\) (set of square symmetric matrices of dimension \\(n\\)). Note, that only a square matrix could be symmetric by definition.\n. . .\nA matrix \\(A \\in \\mathbb{S}^n\\) is called positive (negative) definite if for all \\(x \\neq 0 : x^T Ax &gt; (&lt;) 0\\). We denote this as \\(A \\succ (\\prec) 0\\). The set of such matrices is denoted as \\(\\mathbb{S}^n_{++} (\\mathbb{S}^n_{- -})\\)\n. . .\nA matrix \\(A \\in \\mathbb{S}^n\\) is called positive (negative) semidefinite if for all \\(x : x^T Ax \\geq (\\leq) 0\\). We denote this as \\(A \\succeq (\\preceq) 0\\). The set of such matrices is denoted as \\(\\mathbb{S}^n_{+} (\\mathbb{S}^n_{-})\\)\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it correct, that a positive definite matrix has all positive entries?\n\n\n\n\n. . .\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it correct, that if a matrix is symmetric it should be positive definite?\n\n\n\n\n. . .\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it correct, that if a matrix is positive definite it should be symmetric?\n\n\n\n\n\n\n\n\nLet \\(A\\) be a matrix of size \\(m \\times n\\), and \\(B\\) be a matrix of size \\(n \\times p\\), and let the product \\(AB\\) be: \\[\nC = AB\n\\] then \\(C\\) is a \\(m \\times p\\) matrix, with element \\((i, j)\\) given by: \\[\nc_{ij} = \\sum_{k=1}^n a_{ik}b_{kj}.\n\\]\nThis operation in a naive form requires \\(\\mathcal{O}(n^3)\\) arithmetical operations, where \\(n\\) is usually assumed as the largest dimension of matrices.\n. . .\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it possible to multiply two matrices faster, than \\(\\mathcal{O}(n^3)\\)? How about \\(\\mathcal{O}(n^2)\\), \\(\\mathcal{O}(n)\\)?\n\n\n\n\n\n\n\n\nLet \\(A\\) be a matrix of shape \\(m \\times n\\), and \\(x\\) be \\(n \\times 1\\) vector, then the \\(i\\)-th component of the product: \\[\nz = Ax\n\\] is given by: \\[\nz_i = \\sum_{k=1}^n a_{ik}x_k\n\\]\nThis operation in a naive form requires \\(\\mathcal{O}(n^2)\\) arithmetical operations, where \\(n\\) is usually assumed as the largest dimension of matrices.\nRemember, that:\n\n\\(C = AB \\quad C^T = B^T A^T\\)\n\\(AB \\neq BA\\)\n\\(e^{A} =\\sum\\limits_{k=0}^{\\infty }{1 \\over k!}A^{k}\\)\n\\(e^{A+B} \\neq e^{A} e^{B}\\) (but if \\(A\\) and \\(B\\) are commuting matrices, which means that \\(AB = BA\\), \\(e^{A+B} = e^{A} e^{B}\\))\n\\(\\langle x, Ay\\rangle = \\langle A^T x, y\\rangle\\)\n\n\n\n\nNorm is a qualitative measure of the smallness of a vector and is typically denoted as \\(\\Vert x \\Vert\\).\nThe norm should satisfy certain properties:\n\n\\(\\Vert \\alpha x \\Vert = \\vert \\alpha\\vert \\Vert x \\Vert\\), \\(\\alpha \\in \\mathbb{R}\\)\n\\(\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert\\) (triangle inequality)\nIf \\(\\Vert x \\Vert = 0\\) then \\(x = 0\\)\n\n. . .\nThe distance between two vectors is then defined as \\[\nd(x, y) = \\Vert x - y \\Vert.\n\\] The most well-known and widely used norm is Euclidean norm: \\[\n\\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2},\n\\] which corresponds to the distance in our real life. If the vectors have complex elements, we use their modulus. Euclidean norm, or \\(2\\)-norm, is a subclass of an important class of \\(p\\)-norms:\n\\[\n\\Vert x \\Vert_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}.\n\\]\n\n\n\n\nThere are two very important special cases. The infinity norm, or Chebyshev norm is defined as the element of the maximal absolute value: \\[\n\\Vert x \\Vert_{\\infty} = \\max_i | x_i|\n\\]\n. . .\n\\(L_1\\) norm (or Manhattan distance) which is defined as the sum of modules of the elements of \\(x\\):\n\\[\n\\Vert x \\Vert_1 = \\sum_i |x_i|\n\\]\n. . .\n\\(L_1\\) norm plays a very important role: it all relates to the compressed sensing methods that emerged in the mid-00s as one of the most popular research topics. The code for the picture below is available here:. Check also this video.\n\n\n\nBalls in different norms on a plane\n\n\n\n\n\nIn some sense there is no big difference between matrices and vectors (you can vectorize the matrix), and here comes the simplest matrix norm Frobenius norm: \\[\n\\Vert A \\Vert_F = \\left(\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2\\right)^{1/2}\n\\]\n. . .\nSpectral norm, \\(\\Vert A \\Vert_2\\) is one of the most used matrix norms (along with the Frobenius norm).\n\\[\n\\Vert A \\Vert_2 = \\sup_{x \\ne 0} \\frac{\\Vert A x \\Vert_2}{\\Vert x \\Vert_{2}},\n\\]\nIt can not be computed directly from the entries using a simple formula, like the Frobenius norm, however, there are efficient algorithms to compute it. It is directly related to the singular value decomposition (SVD) of the matrix. It holds\n\\[\n\\Vert A \\Vert_2 = \\sigma_1(A) = \\sqrt{\\lambda_{\\max}(A^TA)}\n\\]\nwhere \\(\\sigma_1(A)\\) is the largest singular value of the matrix \\(A\\).\n\n\n\nThe standard scalar (inner) product between vectors \\(x\\) and \\(y\\) from \\(\\mathbb{R}^n\\) is given by \\[\n\\langle x, y \\rangle = x^T y = \\sum\\limits_{i=1}^n x_i y_i = y^T x =  \\langle y, x \\rangle\n\\]\nHere \\(x_i\\) and \\(y_i\\) are the scalar \\(i\\)-th components of corresponding vectors.\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve, that you can switch the position of a matrix inside a scalar product with transposition: \\(\\langle x, Ay\\rangle = \\langle A^Tx, y\\rangle\\) and \\(\\langle x, yB\\rangle = \\langle xB^T, y\\rangle\\)\n\n\n\n\n\n\n\nThe standard scalar (inner) product between matrices \\(X\\) and \\(Y\\) from \\(\\mathbb{R}^{m \\times n}\\) is given by\n\\[\n\\langle X, Y \\rangle = \\text{tr}(X^T Y) = \\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^n X_{ij} Y_{ij} =  \\text{tr}(Y^T X) =  \\langle Y, X \\rangle\n\\]\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs there any connection between the Frobenious norm \\(\\Vert \\cdot \\Vert_F\\) and scalar product between matrices \\(\\langle \\cdot, \\cdot \\rangle\\)?\n\n\n\n\n\n\n\nA scalar value \\(\\lambda\\) is an eigenvalue of the \\(n \\times n\\) matrix \\(A\\) if there is a nonzero vector \\(q\\) such that \\[\nAq = \\lambda q.\n\\]\nhe vector \\(q\\) is called an eigenvector of \\(A\\). The matrix \\(A\\) is nonsingular if none of its eigenvalues are zero. The eigenvalues of symmetric matrices are all real numbers, while nonsymmetric matrices may have imaginary eigenvalues. If the matrix is positive definite as well as symmetric, its eigenvalues are all positive real numbers.\n\n\n\n\n\n\n\n\n\nTheorem\n\n\n\n\n\n\\[\nA \\succeq (\\succ) 0 \\Leftrightarrow \\text{all eigenvalues of } A \\text{ are } \\geq (&gt;) 0\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\n\\(\\rightarrow\\) Suppose some eigenvalue \\(\\lambda\\) is negative and let \\(x\\) denote its corresponding eigenvector. Then \\[\nAx = \\lambda x \\rightarrow x^T Ax = \\lambda x^T x &lt; 0\n\\] which contradicts the condition of \\(A \\succeq 0\\).\n\\(\\leftarrow\\) For any symmetric matrix, we can pick a set of eigenvectors \\(v_1, \\dots, v_n\\) that form an orthogonal basis of \\(\\mathbb{R}^n\\). Pick any \\(x \\in \\mathbb{R}^n\\). \\[\n\\begin{split}\nx^T A x &= (\\alpha_1 v_1 + \\ldots + \\alpha_n v_n)^T A (\\alpha_1 v_1 + \\ldots + \\alpha_n v_n)\\\\\n&= \\sum \\alpha_i^2 v_i^T A v_i = \\sum \\alpha_i^2 \\lambda_i v_i^T v_i \\geq 0\n\\end{split}\n\\] here we have used the fact that \\(v_i^T v_j = 0\\), for \\(i \\neq j\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose \\(A \\in S_n\\), i.e., \\(A\\) is a real symmetric \\(n \\times n\\) matrix. Then \\(A\\) can be factorized as\n\\[\nA = Q\\Lambda Q^T,\n\\]\n. . .\nwhere \\(Q \\in \\mathbb{R}^{n \\times n}\\) is orthogonal, i.e., satisfies \\(Q^T Q = I\\), and \\(\\Lambda = \\text{diag}(\\lambda_1, \\ldots , \\lambda_n)\\). The (real) numbers \\(\\lambda_i\\) are the eigenvalues of \\(A\\) and are the roots of the characteristic polynomial \\(\\text{det}(A - \\lambda I)\\). The columns of \\(Q\\) form an orthonormal set of eigenvectors of \\(A\\). The factorization is called the spectral decomposition or (symmetric) eigenvalue decomposition of \\(A\\). 2\n. . .\nWe usually order the eigenvalues as \\(\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_n\\). We use the notation \\(\\lambda_i(A)\\) to refer to the \\(i\\)-th largest eigenvalue of \\(A \\in S\\). We usually write the largest or maximum eigenvalue as \\(\\lambda_1(A) = \\lambda_{\\text{max}}(A)\\), and the least or minimum eigenvalue as \\(\\lambda_n(A) = \\lambda_{\\text{min}}(A)\\).\n\n\n\nThe largest and smallest eigenvalues satisfy\n\\[\n\\lambda_{\\text{min}} (A) = \\inf_{x \\neq 0} \\dfrac{x^T Ax}{x^T x}, \\qquad \\lambda_{\\text{max}} (A) = \\sup_{x \\neq 0} \\dfrac{x^T Ax}{x^T x}\n\\]\n. . .\nand consequently \\(\\forall x \\in \\mathbb{R}^n\\) (Rayleigh quotient):\n\\[\n\\lambda_{\\text{min}} (A) x^T x \\leq x^T Ax \\leq \\lambda_{\\text{max}} (A) x^T x\n\\]\n. . .\nThe condition number of a nonsingular matrix is defined as\n\\[\n\\kappa(A) = \\|A\\|\\|A^{-1}\\|\n\\]\n. . .\nIf we use spectral matrix norm, we can get:\n\\[\n\\kappa(A) = \\dfrac{\\sigma_{\\text{max}}(A)}{\\sigma _{\\text{min}}(A)}\n\\]\nIf, moreover, \\(A \\in \\mathbb{S}^n_{++}\\): \\(\\kappa(A) = \\dfrac{\\lambda_{\\text{max}}(A)}{\\lambda_{\\text{min}}(A)}\\)\n\n\n\n\n\n\n\nSuppose \\(A \\in \\mathbb{R}^{m \\times n}\\) with rank \\(A = r\\). Then \\(A\\) can be factored as\n\\[\nA = U \\Sigma V^T\n\\]\n. . .\nwhere \\(U \\in \\mathbb{R}^{m \\times r}\\) satisfies \\(U^T U = I\\), \\(V \\in \\mathbb{R}^{n \\times r}\\) satisfies \\(V^T V = I\\), and \\(\\Sigma\\) is a diagonal matrix with \\(\\Sigma = \\text{diag}(\\sigma_1, ..., \\sigma_r)\\), such that\n. . .\n\\[\n\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r &gt; 0.\n\\]\n. . .\nThis factorization is called the singular value decomposition (SVD) of \\(A\\). The columns of \\(U\\) are called left singular vectors of \\(A\\), the columns of \\(V\\) are right singular vectors, and the numbers \\(\\sigma_i\\) are the singular values. The singular value decomposition can be written as\n\\[\nA = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T,\n\\]\nwhere \\(u_i \\in \\mathbb{R}^m\\) are the left singular vectors, and \\(v_i \\in \\mathbb{R}^n\\) are the right singular vectors.\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nSuppose, matrix \\(A \\in \\mathbb{S}^n_{++}\\). What can we say about the connection between its eigenvalues and singular values?\n\n\n\n\n. . .\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow do the singular values of a matrix relate to its eigenvalues, especially for a symmetric matrix?\n\n\n\n\n\n\n\n\n\nSimple, yet very interesting decomposition is Skeleton decomposition, which can be written in two forms:\n\\[\nA = U V^T \\quad A = \\hat{C}\\hat{A}^{-1}\\hat{R}\n\\]\n. . .\nThe latter expression refers to the fun fact: you can randomly choose \\(r\\) linearly independent columns of a matrix and any \\(r\\) linearly independent rows of a matrix and store only them with the ability to reconstruct the whole matrix exactly.\n. . .\nUse cases for Skeleton decomposition are:\n\nModel reduction, data compression, and speedup of computations in numerical analysis: given rank-\\(r\\) matrix with \\(r \\ll n, m\\) one needs to store \\(\\mathcal{O}((n + m)r) \\ll nm\\) elements.\nFeature extraction in machine learning, where it is also known as matrix factorization\nAll applications where SVD applies, since Skeleton decomposition can be transformed into truncated SVD form.\n\n\n\n\n\n\n\n\nFigure 2: Illustration of Skeleton decomposition\n\n\n\n\n\n\n\n\nOne can consider the generalization of Skeleton decomposition to the higher order data structure, like tensors, which implies representing the tensor as a sum of \\(r\\) primitive tensors.\n\n\n\nIllustration of Canonical Polyadic decomposition\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nNote, that there are many tensor decompositions: Canonical, Tucker, Tensor Train (TT), Tensor Ring (TR), and others. In the tensor case, we do not have a straightforward definition of rank for all types of decompositions. For example, for TT decomposition rank is not a scalar, but a vector.\n\n\n\n\n\n\n\nThe determinant and trace can be expressed in terms of the eigenvalues \\[\n\\text{det} A = \\prod\\limits_{i=1}^n \\lambda_i, \\qquad \\text{tr} A = \\sum\\limits_{i=1}^n \\lambda_i\n\\] The determinant has several appealing (and revealing) properties. For instance,\n\n\\(\\text{det} A = 0\\) if and only if \\(A\\) is singular;\n\\(\\text{det}  AB = (\\text{det} A)(\\text{det}  B)\\);\n\\(\\text{det}  A^{-1} = \\frac{1}{\\text{det} \\ A}\\).\n\n. . .\nDon’t forget about the cyclic property of a trace for arbitrary matrices \\(A, B, C, D\\) (assuming, that all dimensions are consistent):\n\\[\n\\text{tr} (ABCD) = \\text{tr} (DABC) = \\text{tr} (CDAB) = \\text{tr} (BCDA)\n\\]\n. . .\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the determinant of a matrix relate to its invertibility?"
  },
  {
    "objectID": "lectures/1.html#vectors-and-matrices",
    "href": "lectures/1.html#vectors-and-matrices",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "",
    "text": "We will treat all vectors as column vectors by default. The space of real vectors of length \\(n\\) is denoted by \\(\\mathbb{R}^n\\), while the space of real-valued \\(m \\times n\\) matrices is denoted by \\(\\mathbb{R}^{m \\times n}\\). That’s it: 1\n\\[\nx = \\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix} \\quad x^T = \\begin{bmatrix}\nx_1 & x_2 & \\dots & x_n\n\\end{bmatrix} \\quad x \\in \\mathbb{R}^n, x_i \\in \\mathbb{R}\n\\tag{1}\\]\n. . .\nSimilarly, if \\(A \\in \\mathbb{R}^{m \\times n}\\) we denote transposition as \\(A^T \\in \\mathbb{R}^{n \\times m}\\): \\[\nA = \\begin{bmatrix}\na_{11} & a_{12} & \\dots & a_{1n} \\\\\na_{21} & a_{22} & \\dots & a_{2n} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{m1} & a_{m2} & \\dots & a_{mn}\n\\end{bmatrix} \\quad A^T = \\begin{bmatrix}\na_{11} & a_{21} & \\dots & a_{m1} \\\\\na_{12} & a_{22} & \\dots & a_{m2} \\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\na_{1n} & a_{2n} & \\dots & a_{mn}\n\\end{bmatrix} \\quad A \\in \\mathbb{R}^{m \\times n}, a_{ij} \\in \\mathbb{R}\n\\] We will write \\(x \\geq 0\\) and \\(x \\neq 0\\) to indicate componentwise relationships\n\n\n\n\n\n\n\nFigure 1: Equivivalent representations of a vector\n\n\n\n\nA matrix is symmetric if \\(A = A^T\\). It is denoted as \\(A \\in \\mathbb{S}^n\\) (set of square symmetric matrices of dimension \\(n\\)). Note, that only a square matrix could be symmetric by definition.\n. . .\nA matrix \\(A \\in \\mathbb{S}^n\\) is called positive (negative) definite if for all \\(x \\neq 0 : x^T Ax &gt; (&lt;) 0\\). We denote this as \\(A \\succ (\\prec) 0\\). The set of such matrices is denoted as \\(\\mathbb{S}^n_{++} (\\mathbb{S}^n_{- -})\\)\n. . .\nA matrix \\(A \\in \\mathbb{S}^n\\) is called positive (negative) semidefinite if for all \\(x : x^T Ax \\geq (\\leq) 0\\). We denote this as \\(A \\succeq (\\preceq) 0\\). The set of such matrices is denoted as \\(\\mathbb{S}^n_{+} (\\mathbb{S}^n_{-})\\)\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it correct, that a positive definite matrix has all positive entries?\n\n\n\n\n. . .\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it correct, that if a matrix is symmetric it should be positive definite?\n\n\n\n\n. . .\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it correct, that if a matrix is positive definite it should be symmetric?"
  },
  {
    "objectID": "lectures/1.html#matrix-product-matmul",
    "href": "lectures/1.html#matrix-product-matmul",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "",
    "text": "Let \\(A\\) be a matrix of size \\(m \\times n\\), and \\(B\\) be a matrix of size \\(n \\times p\\), and let the product \\(AB\\) be: \\[\nC = AB\n\\] then \\(C\\) is a \\(m \\times p\\) matrix, with element \\((i, j)\\) given by: \\[\nc_{ij} = \\sum_{k=1}^n a_{ik}b_{kj}.\n\\]\nThis operation in a naive form requires \\(\\mathcal{O}(n^3)\\) arithmetical operations, where \\(n\\) is usually assumed as the largest dimension of matrices.\n. . .\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs it possible to multiply two matrices faster, than \\(\\mathcal{O}(n^3)\\)? How about \\(\\mathcal{O}(n^2)\\), \\(\\mathcal{O}(n)\\)?"
  },
  {
    "objectID": "lectures/1.html#matrix-by-vector-product-matvec",
    "href": "lectures/1.html#matrix-by-vector-product-matvec",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "",
    "text": "Let \\(A\\) be a matrix of shape \\(m \\times n\\), and \\(x\\) be \\(n \\times 1\\) vector, then the \\(i\\)-th component of the product: \\[\nz = Ax\n\\] is given by: \\[\nz_i = \\sum_{k=1}^n a_{ik}x_k\n\\]\nThis operation in a naive form requires \\(\\mathcal{O}(n^2)\\) arithmetical operations, where \\(n\\) is usually assumed as the largest dimension of matrices.\nRemember, that:\n\n\\(C = AB \\quad C^T = B^T A^T\\)\n\\(AB \\neq BA\\)\n\\(e^{A} =\\sum\\limits_{k=0}^{\\infty }{1 \\over k!}A^{k}\\)\n\\(e^{A+B} \\neq e^{A} e^{B}\\) (but if \\(A\\) and \\(B\\) are commuting matrices, which means that \\(AB = BA\\), \\(e^{A+B} = e^{A} e^{B}\\))\n\\(\\langle x, Ay\\rangle = \\langle A^T x, y\\rangle\\)"
  },
  {
    "objectID": "lectures/1.html#norms",
    "href": "lectures/1.html#norms",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "",
    "text": "Norm is a qualitative measure of the smallness of a vector and is typically denoted as \\(\\Vert x \\Vert\\).\nThe norm should satisfy certain properties:\n\n\\(\\Vert \\alpha x \\Vert = \\vert \\alpha\\vert \\Vert x \\Vert\\), \\(\\alpha \\in \\mathbb{R}\\)\n\\(\\Vert x + y \\Vert \\leq \\Vert x \\Vert + \\Vert y \\Vert\\) (triangle inequality)\nIf \\(\\Vert x \\Vert = 0\\) then \\(x = 0\\)\n\n. . .\nThe distance between two vectors is then defined as \\[\nd(x, y) = \\Vert x - y \\Vert.\n\\] The most well-known and widely used norm is Euclidean norm: \\[\n\\Vert x \\Vert_2 = \\sqrt{\\sum_{i=1}^n |x_i|^2},\n\\] which corresponds to the distance in our real life. If the vectors have complex elements, we use their modulus. Euclidean norm, or \\(2\\)-norm, is a subclass of an important class of \\(p\\)-norms:\n\\[\n\\Vert x \\Vert_p = \\Big(\\sum_{i=1}^n |x_i|^p\\Big)^{1/p}.\n\\]"
  },
  {
    "objectID": "lectures/1.html#p-norm-of-a-vector",
    "href": "lectures/1.html#p-norm-of-a-vector",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "",
    "text": "There are two very important special cases. The infinity norm, or Chebyshev norm is defined as the element of the maximal absolute value: \\[\n\\Vert x \\Vert_{\\infty} = \\max_i | x_i|\n\\]\n. . .\n\\(L_1\\) norm (or Manhattan distance) which is defined as the sum of modules of the elements of \\(x\\):\n\\[\n\\Vert x \\Vert_1 = \\sum_i |x_i|\n\\]\n. . .\n\\(L_1\\) norm plays a very important role: it all relates to the compressed sensing methods that emerged in the mid-00s as one of the most popular research topics. The code for the picture below is available here:. Check also this video.\n\n\n\nBalls in different norms on a plane"
  },
  {
    "objectID": "lectures/1.html#matrix-norms",
    "href": "lectures/1.html#matrix-norms",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "",
    "text": "In some sense there is no big difference between matrices and vectors (you can vectorize the matrix), and here comes the simplest matrix norm Frobenius norm: \\[\n\\Vert A \\Vert_F = \\left(\\sum_{i=1}^m \\sum_{j=1}^n |a_{ij}|^2\\right)^{1/2}\n\\]\n. . .\nSpectral norm, \\(\\Vert A \\Vert_2\\) is one of the most used matrix norms (along with the Frobenius norm).\n\\[\n\\Vert A \\Vert_2 = \\sup_{x \\ne 0} \\frac{\\Vert A x \\Vert_2}{\\Vert x \\Vert_{2}},\n\\]\nIt can not be computed directly from the entries using a simple formula, like the Frobenius norm, however, there are efficient algorithms to compute it. It is directly related to the singular value decomposition (SVD) of the matrix. It holds\n\\[\n\\Vert A \\Vert_2 = \\sigma_1(A) = \\sqrt{\\lambda_{\\max}(A^TA)}\n\\]\nwhere \\(\\sigma_1(A)\\) is the largest singular value of the matrix \\(A\\)."
  },
  {
    "objectID": "lectures/1.html#scalar-product",
    "href": "lectures/1.html#scalar-product",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "",
    "text": "The standard scalar (inner) product between vectors \\(x\\) and \\(y\\) from \\(\\mathbb{R}^n\\) is given by \\[\n\\langle x, y \\rangle = x^T y = \\sum\\limits_{i=1}^n x_i y_i = y^T x =  \\langle y, x \\rangle\n\\]\nHere \\(x_i\\) and \\(y_i\\) are the scalar \\(i\\)-th components of corresponding vectors.\n\n\n\n\n\n\nExample\n\n\n\n\n\nProve, that you can switch the position of a matrix inside a scalar product with transposition: \\(\\langle x, Ay\\rangle = \\langle A^Tx, y\\rangle\\) and \\(\\langle x, yB\\rangle = \\langle xB^T, y\\rangle\\)"
  },
  {
    "objectID": "lectures/1.html#matrix-scalar-product",
    "href": "lectures/1.html#matrix-scalar-product",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "",
    "text": "The standard scalar (inner) product between matrices \\(X\\) and \\(Y\\) from \\(\\mathbb{R}^{m \\times n}\\) is given by\n\\[\n\\langle X, Y \\rangle = \\text{tr}(X^T Y) = \\sum\\limits_{i=1}^m\\sum\\limits_{j=1}^n X_{ij} Y_{ij} =  \\text{tr}(Y^T X) =  \\langle Y, X \\rangle\n\\]\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nIs there any connection between the Frobenious norm \\(\\Vert \\cdot \\Vert_F\\) and scalar product between matrices \\(\\langle \\cdot, \\cdot \\rangle\\)?"
  },
  {
    "objectID": "lectures/1.html#eigenvectors-and-eigenvalues",
    "href": "lectures/1.html#eigenvectors-and-eigenvalues",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "",
    "text": "A scalar value \\(\\lambda\\) is an eigenvalue of the \\(n \\times n\\) matrix \\(A\\) if there is a nonzero vector \\(q\\) such that \\[\nAq = \\lambda q.\n\\]\nhe vector \\(q\\) is called an eigenvector of \\(A\\). The matrix \\(A\\) is nonsingular if none of its eigenvalues are zero. The eigenvalues of symmetric matrices are all real numbers, while nonsymmetric matrices may have imaginary eigenvalues. If the matrix is positive definite as well as symmetric, its eigenvalues are all positive real numbers."
  },
  {
    "objectID": "lectures/1.html#eigenvectors-and-eigenvalues-1",
    "href": "lectures/1.html#eigenvectors-and-eigenvalues-1",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "",
    "text": "Theorem\n\n\n\n\n\n\\[\nA \\succeq (\\succ) 0 \\Leftrightarrow \\text{all eigenvalues of } A \\text{ are } \\geq (&gt;) 0\n\\]\n\n\n\n\n\n\nProof\n\n\n\n\n\n\n\n\n\\(\\rightarrow\\) Suppose some eigenvalue \\(\\lambda\\) is negative and let \\(x\\) denote its corresponding eigenvector. Then \\[\nAx = \\lambda x \\rightarrow x^T Ax = \\lambda x^T x &lt; 0\n\\] which contradicts the condition of \\(A \\succeq 0\\).\n\\(\\leftarrow\\) For any symmetric matrix, we can pick a set of eigenvectors \\(v_1, \\dots, v_n\\) that form an orthogonal basis of \\(\\mathbb{R}^n\\). Pick any \\(x \\in \\mathbb{R}^n\\). \\[\n\\begin{split}\nx^T A x &= (\\alpha_1 v_1 + \\ldots + \\alpha_n v_n)^T A (\\alpha_1 v_1 + \\ldots + \\alpha_n v_n)\\\\\n&= \\sum \\alpha_i^2 v_i^T A v_i = \\sum \\alpha_i^2 \\lambda_i v_i^T v_i \\geq 0\n\\end{split}\n\\] here we have used the fact that \\(v_i^T v_j = 0\\), for \\(i \\neq j\\)."
  },
  {
    "objectID": "lectures/1.html#eigendecomposition-spectral-decomposition",
    "href": "lectures/1.html#eigendecomposition-spectral-decomposition",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "",
    "text": "Suppose \\(A \\in S_n\\), i.e., \\(A\\) is a real symmetric \\(n \\times n\\) matrix. Then \\(A\\) can be factorized as\n\\[\nA = Q\\Lambda Q^T,\n\\]\n. . .\nwhere \\(Q \\in \\mathbb{R}^{n \\times n}\\) is orthogonal, i.e., satisfies \\(Q^T Q = I\\), and \\(\\Lambda = \\text{diag}(\\lambda_1, \\ldots , \\lambda_n)\\). The (real) numbers \\(\\lambda_i\\) are the eigenvalues of \\(A\\) and are the roots of the characteristic polynomial \\(\\text{det}(A - \\lambda I)\\). The columns of \\(Q\\) form an orthonormal set of eigenvectors of \\(A\\). The factorization is called the spectral decomposition or (symmetric) eigenvalue decomposition of \\(A\\). 2\n. . .\nWe usually order the eigenvalues as \\(\\lambda_1 \\geq \\lambda_2 \\geq \\ldots \\geq \\lambda_n\\). We use the notation \\(\\lambda_i(A)\\) to refer to the \\(i\\)-th largest eigenvalue of \\(A \\in S\\). We usually write the largest or maximum eigenvalue as \\(\\lambda_1(A) = \\lambda_{\\text{max}}(A)\\), and the least or minimum eigenvalue as \\(\\lambda_n(A) = \\lambda_{\\text{min}}(A)\\)."
  },
  {
    "objectID": "lectures/1.html#eigenvalues",
    "href": "lectures/1.html#eigenvalues",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "",
    "text": "The largest and smallest eigenvalues satisfy\n\\[\n\\lambda_{\\text{min}} (A) = \\inf_{x \\neq 0} \\dfrac{x^T Ax}{x^T x}, \\qquad \\lambda_{\\text{max}} (A) = \\sup_{x \\neq 0} \\dfrac{x^T Ax}{x^T x}\n\\]\n. . .\nand consequently \\(\\forall x \\in \\mathbb{R}^n\\) (Rayleigh quotient):\n\\[\n\\lambda_{\\text{min}} (A) x^T x \\leq x^T Ax \\leq \\lambda_{\\text{max}} (A) x^T x\n\\]\n. . .\nThe condition number of a nonsingular matrix is defined as\n\\[\n\\kappa(A) = \\|A\\|\\|A^{-1}\\|\n\\]\n. . .\nIf we use spectral matrix norm, we can get:\n\\[\n\\kappa(A) = \\dfrac{\\sigma_{\\text{max}}(A)}{\\sigma _{\\text{min}}(A)}\n\\]\nIf, moreover, \\(A \\in \\mathbb{S}^n_{++}\\): \\(\\kappa(A) = \\dfrac{\\lambda_{\\text{max}}(A)}{\\lambda_{\\text{min}}(A)}\\)"
  },
  {
    "objectID": "lectures/1.html#singular-value-decomposition",
    "href": "lectures/1.html#singular-value-decomposition",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "",
    "text": "Suppose \\(A \\in \\mathbb{R}^{m \\times n}\\) with rank \\(A = r\\). Then \\(A\\) can be factored as\n\\[\nA = U \\Sigma V^T\n\\]\n. . .\nwhere \\(U \\in \\mathbb{R}^{m \\times r}\\) satisfies \\(U^T U = I\\), \\(V \\in \\mathbb{R}^{n \\times r}\\) satisfies \\(V^T V = I\\), and \\(\\Sigma\\) is a diagonal matrix with \\(\\Sigma = \\text{diag}(\\sigma_1, ..., \\sigma_r)\\), such that\n. . .\n\\[\n\\sigma_1 \\geq \\sigma_2 \\geq \\ldots \\geq \\sigma_r &gt; 0.\n\\]\n. . .\nThis factorization is called the singular value decomposition (SVD) of \\(A\\). The columns of \\(U\\) are called left singular vectors of \\(A\\), the columns of \\(V\\) are right singular vectors, and the numbers \\(\\sigma_i\\) are the singular values. The singular value decomposition can be written as\n\\[\nA = \\sum_{i=1}^{r} \\sigma_i u_i v_i^T,\n\\]\nwhere \\(u_i \\in \\mathbb{R}^m\\) are the left singular vectors, and \\(v_i \\in \\mathbb{R}^n\\) are the right singular vectors."
  },
  {
    "objectID": "lectures/1.html#singular-value-decomposition-1",
    "href": "lectures/1.html#singular-value-decomposition-1",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "",
    "text": "Question\n\n\n\n\n\nSuppose, matrix \\(A \\in \\mathbb{S}^n_{++}\\). What can we say about the connection between its eigenvalues and singular values?\n\n\n\n\n. . .\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow do the singular values of a matrix relate to its eigenvalues, especially for a symmetric matrix?"
  },
  {
    "objectID": "lectures/1.html#skeleton-decomposition",
    "href": "lectures/1.html#skeleton-decomposition",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "",
    "text": "Simple, yet very interesting decomposition is Skeleton decomposition, which can be written in two forms:\n\\[\nA = U V^T \\quad A = \\hat{C}\\hat{A}^{-1}\\hat{R}\n\\]\n. . .\nThe latter expression refers to the fun fact: you can randomly choose \\(r\\) linearly independent columns of a matrix and any \\(r\\) linearly independent rows of a matrix and store only them with the ability to reconstruct the whole matrix exactly.\n. . .\nUse cases for Skeleton decomposition are:\n\nModel reduction, data compression, and speedup of computations in numerical analysis: given rank-\\(r\\) matrix with \\(r \\ll n, m\\) one needs to store \\(\\mathcal{O}((n + m)r) \\ll nm\\) elements.\nFeature extraction in machine learning, where it is also known as matrix factorization\nAll applications where SVD applies, since Skeleton decomposition can be transformed into truncated SVD form.\n\n\n\n\n\n\n\n\nFigure 2: Illustration of Skeleton decomposition"
  },
  {
    "objectID": "lectures/1.html#canonical-tensor-decomposition",
    "href": "lectures/1.html#canonical-tensor-decomposition",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "",
    "text": "One can consider the generalization of Skeleton decomposition to the higher order data structure, like tensors, which implies representing the tensor as a sum of \\(r\\) primitive tensors.\n\n\n\nIllustration of Canonical Polyadic decomposition\n\n\n\n\n\n\n\n\nExample\n\n\n\n\n\nNote, that there are many tensor decompositions: Canonical, Tucker, Tensor Train (TT), Tensor Ring (TR), and others. In the tensor case, we do not have a straightforward definition of rank for all types of decompositions. For example, for TT decomposition rank is not a scalar, but a vector."
  },
  {
    "objectID": "lectures/1.html#determinant-and-trace",
    "href": "lectures/1.html#determinant-and-trace",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "",
    "text": "The determinant and trace can be expressed in terms of the eigenvalues \\[\n\\text{det} A = \\prod\\limits_{i=1}^n \\lambda_i, \\qquad \\text{tr} A = \\sum\\limits_{i=1}^n \\lambda_i\n\\] The determinant has several appealing (and revealing) properties. For instance,\n\n\\(\\text{det} A = 0\\) if and only if \\(A\\) is singular;\n\\(\\text{det}  AB = (\\text{det} A)(\\text{det}  B)\\);\n\\(\\text{det}  A^{-1} = \\frac{1}{\\text{det} \\ A}\\).\n\n. . .\nDon’t forget about the cyclic property of a trace for arbitrary matrices \\(A, B, C, D\\) (assuming, that all dimensions are consistent):\n\\[\n\\text{tr} (ABCD) = \\text{tr} (DABC) = \\text{tr} (CDAB) = \\text{tr} (BCDA)\n\\]\n. . .\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nHow does the determinant of a matrix relate to its invertibility?"
  },
  {
    "objectID": "lectures/1.html#convergence-rate",
    "href": "lectures/1.html#convergence-rate",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "Convergence rate",
    "text": "Convergence rate\n\n\n\nDifference between the convergence speed"
  },
  {
    "objectID": "lectures/1.html#linear-convergence",
    "href": "lectures/1.html#linear-convergence",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "Linear convergence",
    "text": "Linear convergence\nTo compare the performance of algorithms, we must define the terminology for different types of convergence. Let \\(r_k\\) be a sequence of non-negative real numbers that converges to zero. Typically, we have an iterative method producing a sequence of iterates \\(x_k\\) approaching the optimal solution \\(x^*\\), and \\(r_k = \\|x_k - x^*\\|_2\\).\nThe linear convergence of \\(r_k\\) is defined as follows:\nA sequence \\(\\{r_k\\}_{k=m}^\\infty\\) converges linearly with a parameter \\(0 &lt; q &lt; 1\\) if there exists a constant \\(C &gt; 0\\) such that: \\[\nr_k \\leq C q^k, \\quad \\text{for all } k \\geq m.\n\\] If such a \\(q\\) exists, the sequence is said to have linear convergence. The exact lower bound of all \\(q\\) satisfying the inequality is called the rate of linear convergence of the sequence.\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nSuppose, you have two sequences with linear convergence rates \\(q_1 = 0.1\\) and \\(q_2 = 0.7\\), which one is faster?"
  },
  {
    "objectID": "lectures/1.html#linear-convergence-1",
    "href": "lectures/1.html#linear-convergence-1",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "Linear convergence",
    "text": "Linear convergence\n\n\n\n\n\n\nExample\n\n\n\n\n\nLet us have the following sequence:\n\\[\nr_k = \\dfrac{1}{2^k}\n\\]\nOne can immediately conclude, that we have a linear convergence with parameters \\(q = \\dfrac{1}{2}\\) and \\(C = 0\\).\n\n\n\n\n\n\n\n\n\n\nQuestion\n\n\n\n\n\nDetermine the convergence of the following sequence \\[\nr_k = \\dfrac{3}{2^k}\n\\]"
  },
  {
    "objectID": "lectures/1.html#sublinear-convergence",
    "href": "lectures/1.html#sublinear-convergence",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "Sublinear convergence",
    "text": "Sublinear convergence\nIf the sequence \\(r_k\\) converges to zero, but does not have linear convergence, the convergence is said to be sublinear. Sometimes we can consider the following class of sublinear convergence: \\[\n\\| x_{k+1} - x^* \\|_2 \\leq C k^{q},\n\\] where \\(q &lt; 0\\) and \\(0 &lt; C &lt; \\infty\\). Informally, sublinear convergence means the sequence converges slower than any geometric progression."
  },
  {
    "objectID": "lectures/1.html#superlinear-convergence",
    "href": "lectures/1.html#superlinear-convergence",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "Superlinear convergence",
    "text": "Superlinear convergence\nA sequence \\(\\{r_k\\}_{k=m}^\\infty\\) is said to have superlinear convergence if it converges to zero faster than any linearly convergent sequence. Verify, that a sequence \\(\\{r_k\\}_{k=m}^\\infty\\) is superlinear if it converges linearly with the rate \\(q = 0\\).\nFor \\(p &gt; 1\\), a sequence has superlinear convergence of order \\(p\\) if there exists \\(C &gt; 0\\) and \\(0 &lt; q &lt; 1\\) such that: \\[\nr_k \\leq C q^{p^k}, \\quad \\text{for all } k \\geq m.\n\\] When \\(p = 2\\), this is called quadratic convergence.\n. . .\n\n\n\n\n\n\nIllustrative Example\n\n\n\n\n\nSuppose \\(x^* = 1.23456789\\) (the true solution), and the iterative sequence starts with an error \\(r_k = 10^{-3}\\), corresponding to 3 correct significant digits (\\(1.234\\)).\n\nAfter the first iteration: \\[\nr_{k+1} \\approx r_k^2 = (10^{-3})^2 = 10^{-6}.\n\\] Now the error is \\(10^{-6}\\), and we have 6 correct digits (\\(1.23456\\)).\nAfter the second iteration: \\[\nr_{k+2} \\approx r_{k+1}^2 = (10^{-6})^2 = 10^{-12}.\n\\] Now the error is \\(10^{-12}\\), and we have 12 correct digits (\\(1.234567890123\\))."
  },
  {
    "objectID": "lectures/1.html#convergence-rates-practical-observations",
    "href": "lectures/1.html#convergence-rates-practical-observations",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "Convergence rates practical observations:",
    "text": "Convergence rates practical observations:\n\n\\(\\|x_{k+1} - x^*\\|_2 \\leq \\frac{1}{k^{\\frac1p}} \\|x_0 - x^*\\|_2\\) implies sublinear convergence rate\n\\(\\|x_{k+1} - x^*\\|_2 \\leq q \\|x_k - x^*\\|_2\\) implies linear convergence rate, where \\(q&lt;1\\)\n\\(\\|x_{k+1} - x^*\\|_2 \\leq q \\|x_k - x^*\\|_2^2\\) implies quadratic convergence rate, where \\(q\\|x_0 - x^*\\|&lt;1\\)"
  },
  {
    "objectID": "lectures/1.html#root-test",
    "href": "lectures/1.html#root-test",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "Root test",
    "text": "Root test\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nLet \\((r_k)_{k=m}^\\infty\\) be a sequence of non-negative numbers converging to zero, and let \\(\\alpha := \\limsup_{k \\to \\infty} r_k^{1/k}\\). (Note that \\(\\alpha \\geq 0\\).)\n\nIf \\(0 \\leq \\alpha &lt; 1\\), then \\((r_k)_{k=m}^\\infty\\) converges linearly with constant \\(\\alpha\\).\nIn particular, if \\(\\alpha = 0\\), then \\((r_k)_{k=m}^\\infty\\) converges superlinearly.\nIf \\(\\alpha = 1\\), then \\((r_k)_{k=m}^\\infty\\) converges sublinearly.\nThe case \\(\\alpha &gt; 1\\) is impossible.\n\nProof.\n\nlet us show that if \\((r_k)_{k=m}^\\infty\\) converges linearly with constant \\(0 \\leq \\beta &lt; 1\\), then necessarily \\(\\alpha \\leq \\beta\\). Indeed, by the definition of the constant of linear convergence, for any \\(\\varepsilon &gt; 0\\) satisfying \\(\\beta + \\varepsilon &lt; 1\\), there exists \\(C &gt; 0\\) such that \\(r_k \\leq C(\\beta + \\varepsilon)^k\\) for all \\(k \\geq m\\). From this, \\(r_k^{1/k} \\leq C^{1/k}(\\beta + \\varepsilon)\\) for all \\(k \\geq m\\). Passing to the limit as \\(k \\to \\infty\\) and using \\(C^{1/k} \\to 1\\), we obtain \\(\\alpha \\leq \\beta + \\varepsilon\\). Given the arbitrariness of \\(\\varepsilon\\), it follows that \\(\\alpha \\leq \\beta\\).\nThus, in the case \\(\\alpha = 1\\), the sequence \\((r_k)_{k=m}^\\infty\\) cannot have linear convergence according to the above result (proven by contradiction). Since, nevertheless, \\((r_k)_{k=m}^\\infty\\) converges to zero, it must converge sublinearly."
  },
  {
    "objectID": "lectures/1.html#root-test-1",
    "href": "lectures/1.html#root-test-1",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "Root test",
    "text": "Root test\n\n\n\n\n\n\nTheorem\n\n\n\n\n\n\nNow consider the case \\(0 \\leq \\alpha &lt; 1\\). Let \\(\\varepsilon &gt; 0\\) be an arbitrary number such that \\(\\alpha + \\varepsilon &lt; 1\\). According to the properties of the limsup, there exists \\(N \\geq m\\) such that \\(r_k^{1/k} \\leq \\alpha + \\varepsilon\\) for all \\(k \\geq N\\). Hence, \\(r_k \\leq (\\alpha + \\varepsilon)^k\\) for all \\(k \\geq N\\). Therefore, \\((r_k)_{k=m}^\\infty\\) converges linearly with parameter \\(\\alpha + \\varepsilon\\) (it does not matter that the inequality is only valid from the number \\(N\\)). Due to the arbitrariness of \\(\\varepsilon\\), this means that the constant of linear convergence of \\((r_k)_{k=m}^\\infty\\) does not exceed \\(\\alpha\\). Since, as shown above, the constant of linear convergence cannot be less than \\(\\alpha\\), this means that the constant of linear convergence of \\((r_k)_{k=m}^\\infty\\) is exactly \\(\\alpha\\).\nFinally, let’s show that the case \\(\\alpha &gt; 1\\) is impossible. Indeed, suppose \\(\\alpha &gt; 1\\). Then from the definition of limsup, it follows that for any \\(N \\geq m\\), there exists \\(k \\geq N\\) such that \\(r_k^{1/k} \\geq 1\\), and, in particular, \\(r_k \\geq 1\\). But this means that \\(r_k\\) has a subsequence that is bounded away from zero. Hence, \\((r_k)_{k=m}^\\infty\\) cannot converge to zero, which contradicts the condition."
  },
  {
    "objectID": "lectures/1.html#ratio-test",
    "href": "lectures/1.html#ratio-test",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "Ratio test",
    "text": "Ratio test\nLet \\(\\{r_k\\}_{k=m}^\\infty\\) be a sequence of strictly positive numbers converging to zero. Let\n\\[\nq = \\lim_{k \\to \\infty} \\dfrac{r_{k+1}}{r_k}\n\\]\n\nIf there exists \\(q\\) and \\(0 \\leq q &lt;  1\\), then \\(\\{r_k\\}_{k=m}^\\infty\\) has linear convergence with constant \\(q\\).\nIn particular, if \\(q = 0\\), then \\(\\{r_k\\}_{k=m}^\\infty\\) has superlinear convergence.\nIf \\(q\\) does not exist, but \\(q = \\lim\\limits_{k \\to \\infty} \\sup_k \\dfrac{r_{k+1}}{r_k} &lt;  1\\), then \\(\\{r_k\\}_{k=m}^\\infty\\) has linear convergence with a constant not exceeding \\(q\\).\nIf \\(\\lim\\limits_{k \\to \\infty} \\inf_k \\dfrac{r_{k+1}}{r_k} =1\\), then \\(\\{r_k\\}_{k=m}^\\infty\\) has sublinear convergence.\nThe case \\(\\lim\\limits_{k \\to \\infty} \\inf_k \\dfrac{r_{k+1}}{r_k} &gt; 1\\) is impossible.\nIn all other cases (i.e., when \\(\\lim\\limits_{k \\to \\infty} \\inf_k \\dfrac{r_{k+1}}{r_k} &lt;  1 \\leq  \\lim\\limits_{k \\to \\infty} \\sup_k \\dfrac{r_{k+1}}{r_k}\\)) we cannot claim anything concrete about the convergence rate \\(\\{r_k\\}_{k=m}^\\infty\\)."
  },
  {
    "objectID": "lectures/1.html#ratio-test-lemma",
    "href": "lectures/1.html#ratio-test-lemma",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "Ratio test lemma",
    "text": "Ratio test lemma\n\n\n\n\n\n\nTheorem\n\n\n\n\n\nLet \\((r_k)_{k=m}^\\infty\\) be a sequence of strictly positive numbers. (The strict positivity is necessary to ensure that the ratios \\(\\frac{r_{k+1}}{r_k}\\), which appear below, are well-defined.) Then\n\\[\n\\liminf_{k \\to \\infty} \\frac{r_{k+1}}{r_k} \\leq \\liminf_{k \\to \\infty} r_k^{1/k} \\leq \\limsup_{k \\to \\infty} r_k^{1/k} \\leq \\limsup_{k \\to \\infty} \\frac{r_{k+1}}{r_k}.\n\\]\nProof.\n\nThe middle inequality follows from the fact that the liminf of any sequence is always less than or equal to its limsup. Let’s prove the last inequality; the first one is proved analogously.\nDenote \\(L := \\limsup_{k \\to \\infty} \\frac{r_{k+1}}{r_k}\\). If \\(L = +\\infty\\), then the inequality is obviously true, so let’s assume \\(L\\) is finite. Note that \\(L \\geq 0\\), since the ratio \\(\\frac{r_{k+1}}{r_k}\\) is positive for all \\(k \\geq m\\). Let \\(\\varepsilon &gt; 0\\) be an arbitrary number. According to the properties of limsup, there exists \\(N \\geq m\\) such that \\(\\frac{r_{k+1}}{r_k} \\leq L + \\varepsilon\\) for all \\(k \\geq N\\). From here, \\(r_{k+1} \\leq (L + \\varepsilon)r_k\\) for all \\(k \\geq N\\). Applying induction, we get \\(r_k \\leq (L + \\varepsilon)^{k-N}r_N\\) for all \\(k \\geq N\\). Let \\(C := (L + \\varepsilon)^{-N}r_N\\). Then \\(r_k \\leq C(L + \\varepsilon)^k\\) for all \\(k \\geq N\\), from which \\(r_k^{1/k} \\leq C^{1/k}(L + \\varepsilon)\\). Taking the limsup as \\(k \\to \\infty\\) and using \\(C^{1/k} \\to 1\\), we get \\(\\limsup_{k \\to \\infty} r_k^{1/k} \\leq L + \\varepsilon\\). Given the arbitrariness of \\(\\varepsilon\\), it follows that \\(\\limsup_{k \\to \\infty} r_k^{1/k} \\leq L\\)."
  },
  {
    "objectID": "lectures/1.html#summary-1",
    "href": "lectures/1.html#summary-1",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "Summary",
    "text": "Summary\n\n\n\nОпределения\n\nПоложительно определённая матрица.\nЕвклидова норма вектора.\nНеравенство треугольника для нормы.\n\\(p\\)-норма вектора.\nКак выглядит единичный шар в \\(p\\) - норме на плоскости для \\(p=1,2,\\infty\\)?\nНорма Фробениуса для матрицы.\nСпектральная норма матрицы.\nСкалярное произведение двух векторов.\nСкалярное произведение двух матриц, согласованное с нормой Фробениуса.\nСобственные значения матрицы. Спектр матрицы.\nСвязь спектра матрицы и её определенности.\nСпектральное разложение матрицы.\nСингулярное разложение матрицы.\nСвязь определителя и собственных чисел для квадратной матрицы.\nСвязь следа и собственных чисел для квадратной матрицы.\n\n\n\n\nЛинейная сходимость последовательности.\nСублинейная сходимость последовательности.\nСверхлинейная сходимость последовательности.\nКвадратичная сходимость последовательности.\nТест корней для определения скорости сходимости последовательности.\nТест отношений для определения скорости сходимости последовательности.\n\n\nТеоремы\n\nКритерий положительной определенности матрицы через знаки собственных значений матрицы.\nТест корней\nТест отношений"
  },
  {
    "objectID": "lectures/1.html#footnotes",
    "href": "lectures/1.html#footnotes",
    "title": "Basic linear algebra recap. Convergence rates",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA full introduction to applied linear algebra can be found in Introduction to Applied Linear Algebra – Vectors, Matrices, and Least Squares - book by Stephen Boyd & Lieven Vandenberghe, which is indicated in the source. Also, a useful refresher for linear algebra is in Appendix A of the book Numerical Optimization by Jorge Nocedal Stephen J. Wright.↩︎\nA good cheat sheet with matrix decomposition is available at the NLA course website.↩︎"
  }
]